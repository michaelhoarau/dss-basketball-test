{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "from dataiku import spark as dkuspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = SparkContext.getOrCreate()\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "# Read recipe inputs\n",
    "hive_test = dataiku.Dataset(\"hive_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "schema=hive_test.read_schema()\n",
    "index=0\n",
    "key='year'\n",
    "for v in schema:\n",
    "    if str(v['name'])==key: \n",
    "        break\n",
    "    index += 1\n",
    "        \n",
    "print index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'type': u'bigint', u'name': u'year'}, {u'type': u'string', u'name': u'player'}, {u'type': u'double', u'name': u'pts'}]\n",
      "['year', 'player', 'pts']\n",
      "['yearvalue', 'playervalue', 'ptsvalue']\n",
      "{'player': 'playervalue', 'pts': 'ptsvalue', 'year': 'yearvalue'}\n"
     ]
    }
   ],
   "source": [
    "print schema \n",
    "\n",
    "array = []\n",
    "for v in schema:\n",
    "    array.append(str(v['name']))\n",
    "print array\n",
    "\n",
    "\n",
    "array2 = []\n",
    "for v in schema:\n",
    "    array2.append(str(v['name']+\"value\"))\n",
    "print array2\n",
    "\n",
    "dict={}\n",
    "for i in range(len(array)):\n",
    "    dict[array[i]]=array2[i]\n",
    "print dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-c0c0da4d8617>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-c0c0da4d8617>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print schema_array\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print schema\n",
    "\n",
    "schema_array=[]\n",
    "for item in schema:\n",
    "    for k,v in item: \n",
    "        schema_array.append(json.dumps(\"{\"+str(k)+\"}\"+\":\"+str(v)+\"}\")\n",
    "    \n",
    "print schema_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_test_df = dkuspark.get_dataframe(sqlContext, hive_test)\n",
    "\n",
    "# Compute recipe outputs from inputs\n",
    "# TODO: Replace this part by your actual code that computes the output, as a SparkSQL dataframe\n",
    "test_df = hive_test_df # For this sample code, simply copy input to output\n",
    "\n",
    "# Write recipe outputs\n",
    "test = dataiku.Dataset(\"test\")\n",
    "dkuspark.write_with_schema(test, test_df)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_test",
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
